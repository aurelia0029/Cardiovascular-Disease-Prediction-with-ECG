{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personalized VF Prediction Models - Interactive Analysis\n",
    "\n",
    "This notebook provides an interactive interface for:\n",
    "1. Running personalized model experiments\n",
    "2. Analyzing results\n",
    "3. Comparing with generalized models\n",
    "4. Visualizing patient-specific patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Run Personalized Model Experiment\n",
    "\n",
    "**Note:** This may take 5-15 minutes depending on your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Run from command line (recommended)\n",
    "# !python exp_personalized_models.py\n",
    "\n",
    "# Option 2: Import and run (alternative)\n",
    "# from exp_personalized_models import main\n",
    "# main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_latest_results(results_dir='./personalized_models_output'):\n",
    "    \"\"\"Load the most recent personalized model results\"\"\"\n",
    "    json_files = glob(os.path.join(results_dir, 'personalized_results_*.json'))\n",
    "    \n",
    "    if not json_files:\n",
    "        raise FileNotFoundError(f\"No results found in {results_dir}\")\n",
    "    \n",
    "    latest_file = max(json_files, key=os.path.getmtime)\n",
    "    \n",
    "    with open(latest_file, 'r') as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    print(f\"Loaded: {latest_file}\")\n",
    "    return results\n",
    "\n",
    "# Load results\n",
    "results = load_latest_results()\n",
    "\n",
    "print(f\"\\nSuccessful patients: {len(results['successful_patients'])}\")\n",
    "print(f\"Failed patients: {len(results['failed_patients'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Results DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metrics_df(results):\n",
    "    \"\"\"Extract metrics into a pandas DataFrame\"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for patient_id in results['successful_patients']:\n",
    "        result = results['patient_results'][patient_id]\n",
    "        metrics = result['metrics']\n",
    "        \n",
    "        row = {\n",
    "            'patient_id': patient_id,\n",
    "            'accuracy': metrics['accuracy'],\n",
    "            'train_size': result['train_size'],\n",
    "            'test_size': result['test_size'],\n",
    "            'train_onset': result['train_onset'],\n",
    "            'train_normal': result['train_normal'],\n",
    "            'test_onset': result['test_onset'],\n",
    "            'test_normal': result['test_normal']\n",
    "        }\n",
    "        \n",
    "        if metrics.get('both_classes', False):\n",
    "            row.update({\n",
    "                'roc_auc': metrics['roc_auc'],\n",
    "                'onset_precision': metrics['onset_precision'],\n",
    "                'onset_recall': metrics['onset_recall'],\n",
    "                'onset_f1': metrics['onset_f1'],\n",
    "                'normal_precision': metrics['normal_precision'],\n",
    "                'normal_recall': metrics['normal_recall'],\n",
    "                'normal_f1': metrics['normal_f1']\n",
    "            })\n",
    "        \n",
    "        data.append(row)\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "df = extract_metrics_df(results)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nAccuracy:\")\n",
    "print(f\"  Mean:   {df['accuracy'].mean():.4f} ± {df['accuracy'].std():.4f}\")\n",
    "print(f\"  Median: {df['accuracy'].median():.4f}\")\n",
    "print(f\"  Range:  {df['accuracy'].min():.4f} - {df['accuracy'].max():.4f}\")\n",
    "\n",
    "if 'roc_auc' in df.columns:\n",
    "    print(f\"\\nROC-AUC (patients with both classes):\")\n",
    "    print(f\"  Mean:   {df['roc_auc'].mean():.4f} ± {df['roc_auc'].std():.4f}\")\n",
    "    print(f\"  Median: {df['roc_auc'].median():.4f}\")\n",
    "\n",
    "if 'onset_f1' in df.columns:\n",
    "    print(f\"\\nOnset Detection F1-Score:\")\n",
    "    print(f\"  Mean:   {df['onset_f1'].mean():.4f} ± {df['onset_f1'].std():.4f}\")\n",
    "    print(f\"  Median: {df['onset_f1'].median():.4f}\")\n",
    "\n",
    "print(f\"\\nData Distribution:\")\n",
    "print(f\"  Total segments: {df['train_size'].sum() + df['test_size'].sum()}\")\n",
    "print(f\"  Avg train size: {df['train_size'].mean():.1f} ± {df['train_size'].std():.1f}\")\n",
    "print(f\"  Avg test size:  {df['test_size'].mean():.1f} ± {df['test_size'].std():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualization: Performance by Patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Accuracy\n",
    "axes[0, 0].bar(df['patient_id'], df['accuracy'], color='steelblue', alpha=0.7)\n",
    "axes[0, 0].axhline(df['accuracy'].mean(), color='red', linestyle='--', label='Mean')\n",
    "axes[0, 0].set_xlabel('Patient ID')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].set_title('Accuracy by Patient')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# ROC-AUC\n",
    "if 'roc_auc' in df.columns:\n",
    "    axes[0, 1].bar(df['patient_id'], df['roc_auc'], color='coral', alpha=0.7)\n",
    "    axes[0, 1].axhline(df['roc_auc'].mean(), color='red', linestyle='--', label='Mean')\n",
    "    axes[0, 1].set_xlabel('Patient ID')\n",
    "    axes[0, 1].set_ylabel('ROC-AUC')\n",
    "    axes[0, 1].set_title('ROC-AUC by Patient')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Train/Test sizes\n",
    "x = np.arange(len(df))\n",
    "width = 0.35\n",
    "axes[1, 0].bar(x - width/2, df['train_size'], width, label='Train', alpha=0.7)\n",
    "axes[1, 0].bar(x + width/2, df['test_size'], width, label='Test', alpha=0.7)\n",
    "axes[1, 0].set_xlabel('Patient ID')\n",
    "axes[1, 0].set_ylabel('Number of Segments')\n",
    "axes[1, 0].set_title('Train/Test Split Size by Patient')\n",
    "axes[1, 0].set_xticks(x)\n",
    "axes[1, 0].set_xticklabels(df['patient_id'], rotation=45)\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Onset F1-Score\n",
    "if 'onset_f1' in df.columns:\n",
    "    axes[1, 1].bar(df['patient_id'], df['onset_f1'], color='lightgreen', alpha=0.7)\n",
    "    axes[1, 1].axhline(df['onset_f1'].mean(), color='red', linestyle='--', label='Mean')\n",
    "    axes[1, 1].set_xlabel('Patient ID')\n",
    "    axes[1, 1].set_ylabel('F1-Score')\n",
    "    axes[1, 1].set_title('Onset Detection F1-Score by Patient')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy distribution\n",
    "axes[0].hist(df['accuracy'], bins=15, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "axes[0].axvline(df['accuracy'].mean(), color='red', linestyle='--', linewidth=2, label='Mean')\n",
    "axes[0].axvline(df['accuracy'].median(), color='orange', linestyle='--', linewidth=2, label='Median')\n",
    "axes[0].set_xlabel('Accuracy')\n",
    "axes[0].set_ylabel('Number of Patients')\n",
    "axes[0].set_title('Distribution of Accuracy Across Patients')\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# ROC-AUC distribution\n",
    "if 'roc_auc' in df.columns:\n",
    "    axes[1].hist(df['roc_auc'].dropna(), bins=15, color='coral', alpha=0.7, edgecolor='black')\n",
    "    axes[1].axvline(df['roc_auc'].mean(), color='red', linestyle='--', linewidth=2, label='Mean')\n",
    "    axes[1].axvline(df['roc_auc'].median(), color='orange', linestyle='--', linewidth=2, label='Median')\n",
    "    axes[1].set_xlabel('ROC-AUC')\n",
    "    axes[1].set_ylabel('Number of Patients')\n",
    "    axes[1].set_title('Distribution of ROC-AUC Across Patients')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Top and Bottom Performers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TOP 5 PERFORMERS (by Accuracy):\")\n",
    "print(\"=\" * 80)\n",
    "top5 = df.nlargest(5, 'accuracy')[['patient_id', 'accuracy', 'roc_auc', 'onset_f1', 'train_size', 'test_size']]\n",
    "print(top5.to_string(index=False))\n",
    "\n",
    "print(\"\\n\\nBOTTOM 5 PERFORMERS (by Accuracy):\")\n",
    "print(\"=\" * 80)\n",
    "bottom5 = df.nsmallest(5, 'accuracy')[['patient_id', 'accuracy', 'roc_auc', 'onset_f1', 'train_size', 'test_size']]\n",
    "print(bottom5.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze correlation between data size and performance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Train size vs Accuracy\n",
    "axes[0].scatter(df['train_size'], df['accuracy'], alpha=0.6, s=100)\n",
    "axes[0].set_xlabel('Training Set Size')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Training Set Size vs Accuracy')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Add correlation coefficient\n",
    "corr = df['train_size'].corr(df['accuracy'])\n",
    "axes[0].text(0.05, 0.95, f'Correlation: {corr:.3f}', \n",
    "             transform=axes[0].transAxes, verticalalignment='top',\n",
    "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# Onset proportion vs Accuracy\n",
    "df['onset_ratio'] = df['train_onset'] / df['train_size']\n",
    "axes[1].scatter(df['onset_ratio'], df['accuracy'], alpha=0.6, s=100)\n",
    "axes[1].set_xlabel('Onset Proportion in Training')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Onset Proportion vs Accuracy')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "corr2 = df['onset_ratio'].corr(df['accuracy'])\n",
    "axes[1].text(0.05, 0.95, f'Correlation: {corr2:.3f}', \n",
    "             transform=axes[1].transAxes, verticalalignment='top',\n",
    "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV for further analysis\n",
    "output_file = 'personalized_models_summary.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"✓ Summary exported to: {output_file}\")\n",
    "\n",
    "# Create summary statistics file\n",
    "summary_stats = {\n",
    "    'Total Patients': len(df),\n",
    "    'Mean Accuracy': df['accuracy'].mean(),\n",
    "    'Std Accuracy': df['accuracy'].std(),\n",
    "    'Median Accuracy': df['accuracy'].median(),\n",
    "    'Min Accuracy': df['accuracy'].min(),\n",
    "    'Max Accuracy': df['accuracy'].max()\n",
    "}\n",
    "\n",
    "if 'roc_auc' in df.columns:\n",
    "    summary_stats.update({\n",
    "        'Mean ROC-AUC': df['roc_auc'].mean(),\n",
    "        'Std ROC-AUC': df['roc_auc'].std(),\n",
    "        'Mean Onset F1': df['onset_f1'].mean()\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame([summary_stats])\n",
    "summary_file = 'personalized_models_statistics.csv'\n",
    "summary_df.to_csv(summary_file, index=False)\n",
    "print(f\"✓ Statistics exported to: {summary_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Next Steps\n",
    "\n",
    "After reviewing these results, consider:\n",
    "\n",
    "1. **Compare with generalized models**: Run `compare_personalized_vs_generalized.py`\n",
    "2. **Analyze failure cases**: Why did some patients fail? Data quality? Class imbalance?\n",
    "3. **Feature importance**: Which features are most predictive for each patient?\n",
    "4. **Temporal validation**: Try chronological train/test splits instead of random\n",
    "5. **Transfer learning**: Pre-train on all patients, fine-tune on individual\n",
    "\n",
    "### Key Questions to Answer:\n",
    "\n",
    "- Do personalized models outperform generalized models?\n",
    "- Which patients benefit most from personalization?\n",
    "- Is there sufficient data per patient for reliable models?\n",
    "- What is the relationship between data size and performance?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
